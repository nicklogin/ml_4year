{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227579"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r', encoding='utf-8').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TODO\n",
    "    # make lowercase\n",
    "    # replace all punctuation except dots with spaces\n",
    "    # collapse multiple spaces into one '   ' -> ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^\\w\\. ]',' ',text)\n",
    "    text = re.sub(' +',' ',text)\n",
    "    text = re.sub('\\.(?! )',' ',text)\n",
    "    return text\n",
    "\n",
    "text = preprocess_text(text)\n",
    "\n",
    "assert len(text) == 3141169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
    "        self.itos = list(set(''.join(text)))\n",
    "        self.stoi = {j:i for i,j in enumerate(self.itos)}# TODO\n",
    "        text = [[self.stoi[symb] for symb in string] for string in text]\n",
    "        \n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # TODO\n",
    "            # count bigram freqencies in the text\n",
    "            new_token = self.most_common_bigram(text)# most common bigram in the text\n",
    "            new_id = len(self.itos)\n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = new_id\n",
    "            \n",
    "            # find occurences of the new_token in the text and replace them with new_id\n",
    "            text = self.replace_bigram(text, new_token, new_id)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        text = [[self.stoi[symb] for symb in string] for string in text]# TODO tokenize text by symbols using self.stoi\n",
    "        for token_id, token in enumerate(self.itos):\n",
    "            # find occurences of the token in the text and replace them with token_id\n",
    "            if type(token) == tuple:\n",
    "                text = self.replace_bigram(text, token, token_id)\n",
    "        return text\n",
    "    \n",
    "    def most_common_bigram(self, text):\n",
    "        bigrams = [(s[i],s[i+1]) for s in text for i in range(len(s)-1)]\n",
    "        return Counter(bigrams).most_common(1)[0][0]\n",
    "    \n",
    "    def replace_bigram(self, text, bigram, mask):\n",
    "        for sent_id, sent in enumerate(text):\n",
    "            i = 0\n",
    "            while i < len(sent)-1:\n",
    "                if (sent[i], sent[i+1]) == bigram:\n",
    "                    text[sent_id][i] = mask\n",
    "                    text[sent_id].pop(i+1)\n",
    "                i += 1\n",
    "        return text\n",
    "        \n",
    "    \n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        if tok > len(self.itos):\n",
    "            return ''\n",
    "        result = self.itos[tok]\n",
    "        if type(result) == tuple:\n",
    "            return self.decode_token(result[0]) + self.decode_token(result[1])\n",
    "        return result\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))\n",
    "        \n",
    "        \n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "        \n",
    "    \n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "        \n",
    "    \n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.proba = np.full(shape=(self.vocab_size,self.vocab_size,self.vocab_size),\n",
    "                             fill_value=delta)\n",
    "\n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        result = np.array([self.get_proba(a,b,i,tau) for i in range(self.vocab_size)])# TODO\n",
    "        return result\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        result = self.proba[a][b][c] ** (1/tau)/(self.proba[a][b] ** (1/tau)).sum()\n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        # TODO count 3-grams in the text\n",
    "        for sent in text:\n",
    "            sent = [start_token] + sent + [end_token]\n",
    "            for i in range(len(sent)-2):\n",
    "                self.proba[sent[i]][sent[i+1]][sent[i+2]] += 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "lm = LM(vocab_size, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 1.31810193e-03,\n",
       "       2.19683656e-03, 2.19683656e-04, 1.75746924e-03, 2.19683656e-04,\n",
       "       4.39367311e-04, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       3.27328647e-02, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 2.70210896e-02,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 8.78734622e-04,\n",
       "       2.19683656e-04, 6.59050967e-04, 2.19683656e-04, 7.31546573e-02,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 9.77592267e-02,\n",
       "       2.19683656e-04, 2.19683656e-04, 1.53778559e-02, 2.19683656e-04,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       2.19683656e-04, 1.84534271e-02, 2.19683656e-04, 2.19683656e-04,\n",
       "       4.83304042e-03, 1.97715290e-03, 1.01054482e-02, 2.19683656e-04,\n",
       "       2.19683656e-04, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       1.31151142e-01, 3.88840070e-01, 2.41652021e-03, 2.19683656e-04,\n",
       "       9.44639719e-03, 3.95430580e-03, 1.64762742e-02, 2.19683656e-04,\n",
       "       1.97715290e-03, 1.09841828e-03, 2.41652021e-03, 5.00878735e-02,\n",
       "       1.14235501e-02, 4.39367311e-04, 2.19683656e-04, 9.00702988e-03,\n",
       "       2.19683656e-04, 6.59050967e-04, 1.09841828e-03, 2.19683656e-04,\n",
       "       2.02108963e-02, 2.19683656e-04, 2.19683656e-04, 2.19683656e-04,\n",
       "       2.19683656e-04, 2.19683656e-04, 1.09841828e-02, 8.78734622e-04,\n",
       "       4.39367311e-04, 6.59050967e-04, 2.19683656e-04, 8.34797891e-03,\n",
       "       1.09841828e-03, 2.85588752e-03, 2.19683656e-04, 2.19683656e-04,\n",
       "       2.19683656e-04, 5.49209139e-03, 2.19683656e-04, 4.83304042e-03,\n",
       "       2.19683656e-04, 2.19683656e-04, 3.07557118e-03, 2.85588752e-03,\n",
       "       2.19683656e-04, 6.15114236e-03])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.infer(50,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    beam = [((input_seq[-2], input_seq[-1], c),\n",
    "             lm.get_proba(input_seq[-2], input_seq[-1], c, tau=tau)) for c in top_k(lm.infer(input_seq[-2], input_seq[-1],\n",
    "                                                                                           tau=tau),k=k)]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if snt[-1] == end_token:\n",
    "                candidates.append(snt)\n",
    "                candidates_proba.append(snt_proba)# TODO process terminal token\n",
    "            else:    \n",
    "                proba = lm.infer(snt[-2], snt[-1], tau=tau)# probability vector of the next token\n",
    "                best_k = top_k(proba, k=k)# top-k most probable tokens\n",
    "                # TODO update candidates' sequences and corresponding probabilities\n",
    "                for candidate in best_k:\n",
    "                    candidates.append(snt+(candidate,))\n",
    "                    candidates_proba.append(snt_proba*lm.get_proba(snt[-2], snt[-1], candidate, tau=tau))\n",
    "                \n",
    "        beam = most_probable(candidates, candidates_proba, k=k)# select top-k most probable sequences from candidates\n",
    "    return beam\n",
    "\n",
    "top_k = lambda x, k: np.argsort(x)[::-1][:k]\n",
    "\n",
    "def most_probable(candidates, probas, k):\n",
    "    return [(candidates[i], probas[i]) for i in top_k(probas, k=k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse with a smill  0.6841029778083697\n",
      "horse was not been sa 0.04177810773487709\n",
      "horse was sold not be 0.020091172233032518\n",
      "horse when said not  0.01957560537478095\n",
      "horse the countess mary  0.012419496299872258\n",
      "horse who had been sa 0.010867744904136322\n",
      "horse with his fack  0.007780060616618095\n",
      "horse the counderstand wi 0.007335131282565677\n",
      "horse the cound him and s 0.007187195139002834\n",
      "horse the cound him and w 0.005751116965404067\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for seq, proba in result:\n",
    "    print(bpe.decode(input1)+bpe.decode(seq[2:]), proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here with a smill 0.7304666014228682\n",
      "here was not been s 0.0487404790163391\n",
      "here was sold not b 0.022797616216872155\n",
      "here when said no 0.020902320798071274\n",
      "here who had been s 0.012678867502004658\n",
      "here was she was not b 0.009807273664700425\n",
      "here with his heas 0.009110050541107704\n",
      "here with his fack 0.008307338307196785\n",
      "here with his hear  0.007996051526025703\n",
      "here was not seemp 0.0064405700451872685\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for seq, proba in result:\n",
    "    print(bpe.decode(input1)+bpe.decode(seq[2:]), proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what 0.02443609022556391\n",
      "whated 0.011341031957494033\n",
      "whated to him 0.0002072659731327774\n",
      "what theight 5.717776867622187e-05\n",
      "whated to himself 1.6978380760763765e-05\n",
      "whated to himself it  1.6938712102523908e-06\n",
      "whated to himself i  1.4867449897241193e-06\n",
      "whated to himself sa 9.252974424477093e-07\n",
      "whated to himself he ha 8.899626578301803e-07\n",
      "whated to himself he w 8.326777051422607e-07\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for seq, proba in result:\n",
    "    print(bpe.decode(input1)+bpe.decode(seq[2:]), proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gun and with a smill 0.24096428447585005\n",
      "gun been said not  0.1624530112505025\n",
      "gun and said not been 0.10901816275155284\n",
      "gun and so mussion of  0.03893029842345072\n",
      "gun but was not been 0.03675414427294098\n",
      "gun and so mussion hi 0.03249012096768254\n",
      "gun and so must been  0.031565015712851884\n",
      "gun said not been  0.025207649072660532\n",
      "gun and said not see 0.021695841113877797\n",
      "gun but was sold no 0.02008458233790684\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for seq, proba in result:\n",
    "    print(bpe.decode(input1)+bpe.decode(seq[2:]), proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.090930305288339"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(snt, lm):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for i in range(2,len(snt)):\n",
    "        result += np.log(lm.get_proba(snt[i-2], snt[i-1], snt[i]))\n",
    "    result = result * -1/(len(snt)-2)\n",
    "    return 2**result\n",
    "\n",
    "perplexity(tokenized_text[0], lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
